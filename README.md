# llm_locally
Test what LLM + RAG setup can run  locally using 8G VRAM GPU
This can run in docker.